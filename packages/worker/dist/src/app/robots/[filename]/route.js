"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.GET = GET;
const server_1 = require("next/server");
const supabase_js_1 = require("@supabase/supabase-js");
const supabase = (0, supabase_js_1.createClient)(process.env.NEXT_PUBLIC_SUPABASE_URL, process.env.SUPABASE_SERVICE_ROLE_KEY);
async function GET(request, { params }) {
    try {
        console.log(' [SEOAGENT ROBOTS] Serving robots.txt for filename:', params.filename);
        // Extract domain from filename (remove .txt extension)
        const domain = params.filename.replace(/\.txt$/, '');
        if (!domain) {
            console.error(' [SEOAGENT ROBOTS] No domain extracted from filename');
            return new server_1.NextResponse('Invalid robots.txt filename', { status: 400 });
        }
        console.log(' [SEOAGENT ROBOTS] Looking up domain:', domain);
        // Look up website by domain using various format variations
        const { data: website, error: websiteError } = await supabase
            .from('websites')
            .select('*')
            .or(`domain.eq.${domain},domain.eq.sc-domain:${domain},domain.eq.https://${domain},domain.eq.http://${domain},domain.eq.https://www.${domain},cleaned_domain.eq.${domain}`)
            .limit(1)
            .single();
        if (websiteError || !website) {
            console.log(' [SEOAGENT ROBOTS] Website not found, generating basic robots.txt for:', domain);
            // Generate a basic robots.txt for unknown domains
            const basicRobotsTxt = `User-agent: *
Allow: /

# Sitemap reference
Sitemap: https://${domain}/sitemap.xml

# Generated by SEOAgent.com - Automated SEO for modern websites
# Visit https://seoagent.com to optimize your site&apos;s SEO automatically`;
            return new server_1.NextResponse(basicRobotsTxt, {
                headers: {
                    'Content-Type': 'text/plain',
                    'Cache-Control': 'public, max-age=86400',
                    'X-Generated-By': 'SEOAgent-Basic'
                }
            });
        }
        console.log(' [SEOAGENT ROBOTS] Found website:', website.domain, 'for user:', website.user_token);
        // Check if there's a custom robots.txt configuration for this website
        const { data: robotsConfig, error: robotsError } = await supabase
            .from('seo_configurations')
            .select('robots_txt_content, robots_txt_rules')
            .eq('user_token', website.user_token)
            .or(`domain.eq.${website.domain},site_url.eq.${website.domain}`)
            .single();
        let robotsTxtContent = '';
        if (!robotsError && robotsConfig?.robots_txt_content) {
            console.log(' [SEOAGENT ROBOTS] Using custom robots.txt configuration');
            robotsTxtContent = robotsConfig.robots_txt_content;
        }
        else {
            console.log(' [SEOAGENT ROBOTS] Generating optimized robots.txt');
            // Determine the base URL for sitemap reference
            const baseUrl = website.domain.startsWith('http')
                ? website.domain
                : `https://${website.domain.replace(/^sc-domain:/, '')}`;
            // Generate optimized robots.txt based on website analysis
            robotsTxtContent = `User-agent: *
Allow: /

# Block common admin and system paths
Disallow: /admin/
Disallow: /wp-admin/
Disallow: /wp-content/uploads/
Disallow: /.well-known/
Disallow: /api/
Disallow: /_next/
Disallow: /node_modules/

# Block search and filtering URLs that create duplicate content
Disallow: /*?*search*
Disallow: /*?*filter*
Disallow: /*?*sort*

# Allow important crawlers
User-agent: Googlebot
Allow: /

User-agent: Bingbot
Allow: /

User-agent: facebookexternalhit
Allow: /

# Sitemap reference  
Sitemap: ${baseUrl}/sitemap.xml

# Generated by SEOAgent.com - Automated SEO for modern websites
# Visit https://seoagent.com to optimize your site&apos;s SEO automatically`;
        }
        // Check for any specific robots.txt customizations from technical SEO analysis
        try {
            const baseUrl = process.env.NEXT_PUBLIC_APP_URL || 'http://localhost:3000';
            const response = await fetch(`${baseUrl}/api/technical-seo/robots-analysis`, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                    userToken: website.user_token,
                    siteUrl: website.domain
                })
            });
            if (response.ok) {
                const analysisData = await response.json();
                if (analysisData.success && analysisData.optimizedRobotsTxt) {
                    console.log(' [SEOAGENT ROBOTS] Using technical SEO optimized robots.txt');
                    robotsTxtContent = analysisData.optimizedRobotsTxt;
                }
            }
        }
        catch (analysisError) {
            console.log(' [SEOAGENT ROBOTS] Could not get robots analysis, using default:', analysisError);
        }
        return new server_1.NextResponse(robotsTxtContent, {
            headers: {
                'Content-Type': 'text/plain',
                'Cache-Control': 'public, max-age=86400',
                'X-Generated-By': 'SEOAgent-Optimized'
            }
        });
    }
    catch (error) {
        console.error(' [SEOAGENT ROBOTS] Error serving robots.txt:', error);
        // Return a basic error robots.txt instead of JSON error
        const domain = params.filename.replace(/\.txt$/, '');
        const errorRobotsTxt = `User-agent: *
Allow: /

Sitemap: https://${domain}/sitemap.xml

# Error occurred generating robots.txt
# Generated by SEOAgent.com`;
        return new server_1.NextResponse(errorRobotsTxt, {
            status: 500,
            headers: {
                'Content-Type': 'text/plain',
                'Cache-Control': 'public, max-age=300',
                'X-Generated-By': 'SEOAgent-Error'
            }
        });
    }
}
