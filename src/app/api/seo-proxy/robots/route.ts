import { NextRequest, NextResponse } from 'next/server';
import { createClient } from '@supabase/supabase-js';

const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!
);

export async function GET(request: NextRequest) {
  try {
    const { searchParams } = new URL(request.url);
    const domain = searchParams.get('domain');
    const userToken = searchParams.get('token');

    if (!domain || !userToken) {
      return NextResponse.json(
        { error: 'Missing required parameters: domain and token' },
        { status: 400 }
      );
    }

    console.log(`[SEO PROXY] Serving robots.txt for domain: ${domain}`);

    // Get stored robots content from database
    const { data: seoContent, error } = await supabase
      .from('hosting_seo_content')
      .select('sitemap_content, robots_content, updated_at')
      .eq('user_token', userToken)
      .eq('domain', domain)
      .eq('status', 'active')
      .order('updated_at', { ascending: false })
      .limit(1)
      .single();

    if (error || !seoContent?.robots_content) {
      console.error('[SEO PROXY] Robots.txt not found:', error);
      
      // Return a basic robots.txt if none exists
      const basicRobots = `User-agent: *
Allow: /

Sitemap: https://${domain}/sitemap.xml

# Generated by SEOAgent.com
# ${new Date().toISOString()}`;

      return new Response(basicRobots, {
        headers: {
          'Content-Type': 'text/plain',
          'Cache-Control': 'public, max-age=3600',
          'X-SEOAgent-Generated': new Date().toISOString(),
          'X-SEOAgent-Domain': domain,
          'X-SEOAgent-Source': 'proxy-fallback'
        }
      });
    }

    // Return the stored robots.txt content
    return new Response(seoContent.robots_content, {
      headers: {
        'Content-Type': 'text/plain',
        'Cache-Control': 'public, max-age=3600',
        'X-SEOAgent-Generated': seoContent.updated_at,
        'X-SEOAgent-Domain': domain,
        'X-SEOAgent-Source': 'proxy-database'
      }
    });

  } catch (error) {
    console.error('[SEO PROXY] Robots.txt proxy error:', error);
    return NextResponse.json(
      { error: 'Internal server error serving robots.txt' },
      { status: 500 }
    );
  }
}

// Store robots.txt content (used by redirect deployment method)
export async function POST(request: NextRequest) {
  try {
    const { domain, userToken, robotsContent, provider } = await request.json();

    if (!domain || !userToken || !robotsContent) {
      return NextResponse.json(
        { error: 'Missing required parameters' },
        { status: 400 }
      );
    }

    console.log(`[SEO PROXY] Storing robots.txt content for domain: ${domain}`);

    // Get existing record first to preserve sitemap_content
    const { data: existing } = await supabase
      .from('hosting_seo_content')
      .select('sitemap_content')
      .eq('user_token', userToken)
      .eq('domain', domain)
      .single();

    // Upsert robots content while preserving sitemap content
    const { data, error } = await supabase
      .from('hosting_seo_content')
      .upsert({
        user_token: userToken,
        domain,
        sitemap_content: existing?.sitemap_content || null,
        robots_content: robotsContent,
        provider: provider || 'unknown',
        status: 'active',
        updated_at: new Date().toISOString()
      }, {
        onConflict: 'user_token,domain'
      })
      .select()
      .single();

    if (error) {
      console.error('[SEO PROXY] Error storing robots.txt content:', error);
      return NextResponse.json(
        { error: 'Failed to store robots.txt content' },
        { status: 500 }
      );
    }

    return NextResponse.json({
      success: true,
      message: 'Robots.txt content stored successfully',
      proxyUrl: `${process.env.NEXT_PUBLIC_APP_URL}/api/seo-proxy/robots?domain=${domain}&token=${userToken}`,
      storedAt: data.updated_at
    });

  } catch (error) {
    console.error('[SEO PROXY] Store robots.txt error:', error);
    return NextResponse.json(
      { error: 'Internal server error storing robots.txt' },
      { status: 500 }
    );
  }
}